{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "BY94vHokTR_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bioc"
      ],
      "metadata": {
        "id": "prEhX8ThXa3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import spacy\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "dataset = []\n",
        "\n",
        "global_id2label = {0: 'O', 1: 'B-ENT', 2: 'I-ENT'}\n",
        "\n",
        "prefix2id = {'O': 0, 'B': 1, 'I': 2, 'S': 1, 'E':2}\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = [tok.text for tok in nlp.make_doc(text)]\n",
        "    return tokens\n",
        "\n",
        "def tokenize_and_match(text):\n",
        "    doc = nlp.make_doc(text)\n",
        "    tokens = []\n",
        "    s2t = {}\n",
        "    e2t = {}\n",
        "    for t, tok in enumerate(doc):\n",
        "        tokens.append(tok.text)\n",
        "        s = tok.idx\n",
        "        s2t[s] = t\n",
        "        e = tok.idx+len(tok.text)\n",
        "        e2t[e] = t\n",
        "    return tokens, s2t, e2t\n",
        "\n",
        "def search_span(text, label, s2t, e2t):\n",
        "    sts = []\n",
        "    ets = []\n",
        "    try:\n",
        "        searches = re.finditer(re.escape(label), text, re.IGNORECASE)\n",
        "    except:\n",
        "        return sts, ets\n",
        "    for search in searches:\n",
        "        s, e = search.span()\n",
        "        if s in s2t and e in e2t:\n",
        "            st = s2t[s]\n",
        "            et = e2t[e]\n",
        "            sts.append(st)\n",
        "            ets.append(et)\n",
        "    return sts, ets\n",
        "\n",
        "def handle_dataset(example, id2ent, id2global_label, dataset = [], curr_ents=[]):\n",
        "    tokens = example['tokens']\n",
        "    if 'tags' in example:\n",
        "        old_tags = example['tags']\n",
        "    elif 'ner_tags' in example:\n",
        "        old_tags = example['ner_tags']\n",
        "\n",
        "    tags = []\n",
        "    start = 0\n",
        "    label = 'other'\n",
        "    end = 0\n",
        "    prev_id = 0\n",
        "    for i, id in enumerate(old_tags):\n",
        "        ent = id2ent[id]\n",
        "        if ent not in curr_ents:\n",
        "            continue\n",
        "        global_id = id2global_label[id]\n",
        "        if prev_id == 2 and global_id!=2:\n",
        "            tags.append([start, end, label])\n",
        "\n",
        "        if global_id==1:\n",
        "            start = i\n",
        "            end = i\n",
        "            label = ent\n",
        "        elif global_id==2:\n",
        "            end+=1\n",
        "        prev_id = global_id\n",
        "\n",
        "    if prev_id:\n",
        "        tags.append([start, end, label])\n",
        "\n",
        "    result = {\"tokenized_text\": tokens, \"ner\": tags}\n",
        "    dataset.append(result)\n",
        "    return example\n",
        "\n",
        "def process_ncbi():\n",
        "    ncbi = load_dataset('ncbi_disease')\n",
        "\n",
        "    label2id = {'O': 0, 'B-Disease': 1, 'I-Disease': 2}\n",
        "\n",
        "    label2ent = {'O': 'other', 'Disease': 'disease'}\n",
        "\n",
        "    id2label = {id:label for label, id in label2id.items()}\n",
        "    id2global_label = {id:prefix2id[label.split('-')[0]] for id, label in id2label.items()}\n",
        "    id2ent = {id: label2ent[label.split('-')[-1]] for id, label in id2label.items()}\n",
        "\n",
        "    all_ents = ['disease']\n",
        "\n",
        "    ncbi_gliner_train = []\n",
        "    ncbi['train'].map(handle_dataset, fn_kwargs = {\"id2ent\":id2ent, \"id2global_label\": id2global_label, \"curr_ents\": all_ents, \"dataset\": ncbi_gliner_train})\n",
        "\n",
        "    ncbi_gliner_test = []\n",
        "    ncbi['test'].map(handle_dataset, fn_kwargs = {\"id2ent\":id2ent, \"id2global_label\": id2global_label, \"curr_ents\": all_ents, \"dataset\": ncbi_gliner_test})\n",
        "\n",
        "    ncbi_gliner_validation = []\n",
        "    ncbi['validation'].map(handle_dataset, fn_kwargs = {\"id2ent\":id2ent, \"id2global_label\": id2global_label, \"curr_ents\": all_ents, \"dataset\": ncbi_gliner_validation})\n",
        "\n",
        "    return ncbi_gliner_train, ncbi_gliner_test, ncbi_gliner_validation\n",
        "\n",
        "\n",
        "def process_bc5cdr():\n",
        "    bc5cdr = load_dataset('tner/bc5cdr')\n",
        "\n",
        "    label2id = {\"O\": 0, \"B-Chemical\": 1, \"B-Disease\": 2, \"I-Disease\": 3, \"I-Chemical\": 4}\n",
        "\n",
        "    label2ent = {'O': 'other', 'Chemical': 'chemical', 'Disease': 'disease'}\n",
        "\n",
        "    id2label = {id:label for label, id in label2id.items()}\n",
        "    id2global_label = {id:prefix2id[label.split('-')[0]] for id, label in id2label.items()}\n",
        "    id2ent = {id: label2ent[label.split('-')[-1]] for id, label in id2label.items()}\n",
        "\n",
        "    all_ents = list(label2ent.values())\n",
        "\n",
        "    bc5cdr_gliner_train = []\n",
        "    bc5cdr['train'].map(handle_dataset, fn_kwargs = {\"id2ent\":id2ent, \"id2global_label\": id2global_label, \"curr_ents\": all_ents, \"dataset\": bc5cdr_gliner_train})\n",
        "\n",
        "    bc5cdr_gliner_test = []\n",
        "    bc5cdr['test'].map(handle_dataset, fn_kwargs = {\"id2ent\":id2ent, \"id2global_label\": id2global_label, \"curr_ents\": all_ents, \"dataset\": bc5cdr_gliner_test})\n",
        "\n",
        "    bc5cdr_gliner_validation = []\n",
        "    bc5cdr['validation'].map(handle_dataset, fn_kwargs = {\"id2ent\":id2ent, \"id2global_label\": id2global_label, \"curr_ents\": all_ents, \"dataset\": bc5cdr_gliner_validation})\n",
        "\n",
        "    return bc5cdr_gliner_train, bc5cdr_gliner_test, bc5cdr_gliner_validation\n",
        "\n"
      ],
      "metadata": {
        "id": "HAR3-EuQRwzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ncbi_train, ncbi_test, ncbi_val = process_ncbi()"
      ],
      "metadata": {
        "id": "h4fqVzg9UTUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bc5cdr_gliner_train, bc5cdr_gliner_test, bc5cdr_gliner_validation = process_bc5cdr()"
      ],
      "metadata": {
        "id": "aDFCLMJLYwpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bigbio_proc_example(example, ent2label, dataset=[]):\n",
        "    text = ' '.join([passage['text'][0] for passage in example['passages']])\n",
        "    tokens, s2t, e2t = tokenize_and_match(text)\n",
        "\n",
        "    entities = example['entities']\n",
        "    tags = []\n",
        "    for id, ent in enumerate(entities):\n",
        "        offset = ent['offsets'][0]\n",
        "\n",
        "        start = offset[0]\n",
        "        end = offset[1]\n",
        "\n",
        "        type_ = ent['type']\n",
        "        ent_label = ent2label[type_]\n",
        "\n",
        "        if start in s2t and end in e2t:\n",
        "            st = s2t[start]\n",
        "            et = e2t[end]\n",
        "            if st-et<0:\n",
        "                continue\n",
        "            tags.append([st, et, ent_label])\n",
        "\n",
        "    dataset.append({\"tokenized_text\": tokens, \"ner\": tags})\n",
        "    return example\n",
        "\n",
        "def get_bigbio_dicts(dataset):\n",
        "    all_entities = set()\n",
        "\n",
        "    for example in dataset['train']:\n",
        "        for ent in example['entities']:\n",
        "            all_entities.add(ent['type'])\n",
        "\n",
        "\n",
        "    if 'test' in dataset:\n",
        "        for example in dataset['test']:\n",
        "            for ent in example['entities']:\n",
        "                all_entities.add(ent['type'])\n",
        "\n",
        "\n",
        "    if 'validation' in dataset:\n",
        "        for example in dataset['validation']:\n",
        "            for ent in example['entities']:\n",
        "                all_entities.add(ent['type'])\n",
        "\n",
        "\n",
        "    ent2label = {ent:' '.join(ent.split('_')).lower() for ent in all_entities}\n",
        "    ent2label[\"\"] = 'other'\n",
        "\n",
        "    all_ents = list(ent2label.values())\n",
        "\n",
        "    return all_ents, ent2label\n",
        "\n",
        "def process_chia():\n",
        "    bigbio = load_dataset('bigbio/chia', 'chia_bigbio_kb')\n",
        "    all_ents, ent2label = get_bigbio_dicts(bigbio)\n",
        "\n",
        "    gliner_dataset = []\n",
        "    bigbio.map(bigbio_proc_example, fn_kwargs={\"ent2label\": ent2label, \"dataset\": gliner_dataset})\n",
        "\n",
        "    return gliner_dataset\n",
        "\n",
        "\n",
        "def process_biored():\n",
        "    biored = load_dataset('bigbio/biored', 'biored_bigbio_kb')\n",
        "\n",
        "    all_entities = set()\n",
        "\n",
        "    for example in biored['train']:\n",
        "        for ent in example['entities']:\n",
        "            all_entities.add(ent['type'])\n",
        "\n",
        "    ent2label = {\n",
        "        'CellLine': 'cell line',\n",
        "        'ChemicalEntity': 'chemical entity',\n",
        "        'DiseaseOrPhenotypicFeature': 'disease or phenotype',\n",
        "        'GeneOrGeneProduct': 'gene or gene product',\n",
        "        'OrganismTaxon': 'organism',\n",
        "        \"SequenceVariant\": 'sequence variant'\n",
        "    }\n",
        "    all_ents = list(ent2label.values())\n",
        "\n",
        "    biored_gliner_train = []\n",
        "    biored['train'].map(bigbio_proc_example, fn_kwargs={\"ent2label\": ent2label, \"dataset\": biored_gliner_train})\n",
        "\n",
        "    biored_gliner_test = []\n",
        "    biored['test'].map(bigbio_proc_example, fn_kwargs={\"ent2label\": ent2label, \"dataset\": biored_gliner_test})\n",
        "\n",
        "    biored_gliner_validation = []\n",
        "    biored['validation'].map(bigbio_proc_example, fn_kwargs={\"ent2label\": ent2label, \"dataset\": biored_gliner_validation})\n",
        "\n",
        "    return biored_gliner_train, biored_gliner_test, biored_gliner_validation\n"
      ],
      "metadata": {
        "id": "mvG3nnEfVSd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chia = process_chia()"
      ],
      "metadata": {
        "id": "Xp5G3PhZVt_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biored_gliner_train, biored_gliner_test, biored_gliner_validation = process_biored()"
      ],
      "metadata": {
        "id": "STZAeJLpW7-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def annotate_bio(example, label2ent, dataset = []):\n",
        "    try:\n",
        "      text = example['text']\n",
        "      ents = example['entities']\n",
        "\n",
        "      tokens, s2t, e2t = tokenize_and_match(text)\n",
        "      tags = []\n",
        "      prev_ent = ents[0]\n",
        "      for id, ent in enumerate(ents):\n",
        "          es = ent['start']\n",
        "          ee = ent['end']\n",
        "          ts = 0\n",
        "          te = 0\n",
        "\n",
        "          if es in s2t:\n",
        "              ts = s2t[es]\n",
        "          if ee in e2t:\n",
        "              te = e2t[ee]\n",
        "\n",
        "          if ts==0:\n",
        "              continue\n",
        "          elif ts>te:\n",
        "              te = ts\n",
        "          if ent['class'] in label2ent:\n",
        "              curr_ent = label2ent[ent['class']]\n",
        "              tags.append([ts, te, curr_ent])\n",
        "\n",
        "      dataset.append({\"tokenized_text\": tokens, \"ner\": tags})\n",
        "      return example\n",
        "    except:\n",
        "        return None\n",
        "def process_biomed_ner():\n",
        "    biomed_ner = load_dataset(\"knowledgator/biomed_NER\")['train']\n",
        "\n",
        "    classes = [\"CHEMICALS\", \"CLINICAL DRUG\", \"BODY SUBSTANCE\", \"ANATOMICAL STRUCTURE\", \"CELLS AND THEIR COMPONENTS\",\n",
        "        \"GENE AND GENE PRODUCTS\", \"INTELLECTUAL PROPERTY\", \"LANGUAGE\", \"REGULATION OR LAW\",\n",
        "        \"GEOGRAPHICAL AREAS\", \"ORGANISM\", \"GROUP\", \"PERSON\", \"ORGANIZATION\", \"PRODUCT\", \"LOCATION\", \"PHENOTYPE\",\n",
        "          \"DISORDER\", \"SIGNALING MOLECULES\", \"EVENT\", \"MEDICAL PROCEDURE\", \"ACTIVITY\", \"FUNCTION\", \"MONEY\"]\n",
        "    label2id = {l:id for id, l in enumerate(classes)}\n",
        "    id2label = {v:k for k, v in label2id.items()}\n",
        "    label2ent = {label: label.lower() for label in label2id.keys()}\n",
        "\n",
        "    biomed_ner_dataset = []\n",
        "    biomed_ner.map(annotate_bio, fn_kwargs={\"label2ent\": label2ent, \"dataset\": biomed_ner_dataset},\n",
        "                                                                                  keep_in_memory=True,\n",
        "                                                                                  load_from_cache_file=False)\n",
        "    return biomed_ner_dataset"
      ],
      "metadata": {
        "id": "3o7cCREsZwuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biomed_ner_dataset = process_biomed_ner()"
      ],
      "metadata": {
        "id": "BKCdksY1aA0t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}