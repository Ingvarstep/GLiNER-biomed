{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import DebertaV2TokenizerFast\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def analyze_data(data, tokenizer):\n",
    "    num_instances = len(data)\n",
    "    token_lengths = [len(tokenizer.tokenize(instance['tokenized_text'], is_split_into_words=True, add_special_tokens=False)) for instance in data]\n",
    "    num_entities = [len(instance['ner']) for instance in data]\n",
    "    entity_types = Counter([entity[2] for instance in data for entity in instance['ner']])\n",
    "    negative_types = Counter([neg for instance in data for neg in instance.get('negatives', [])])\n",
    "\n",
    "    return {\n",
    "        'num_instances': num_instances,\n",
    "        'token_lengths': token_lengths,\n",
    "        'num_entities': num_entities,\n",
    "        'entity_types': entity_types,\n",
    "        'negative_types': negative_types\n",
    "    }\n",
    "\n",
    "def collect_summary_statistics(stats, split_name):\n",
    "    return {\n",
    "        'Split': split_name,\n",
    "        'Number of Instances': stats['num_instances'],\n",
    "        'Average Token Length': sum(stats['token_lengths']) / len(stats['token_lengths']),\n",
    "        'Average Number of Entities': sum(stats['num_entities']) / len(stats['num_entities'])\n",
    "    }\n",
    "\n",
    "def collect_detailed_statistics(stats, split_name):\n",
    "    detailed_stats = []\n",
    "    for entity_type, count in stats['entity_types'].items():\n",
    "        detailed_stats.append({\n",
    "            'Split': split_name,\n",
    "            'Entity Type': entity_type,\n",
    "            'Number of Positive Instances': count,\n",
    "            'Number of Negative Instances': stats['negative_types'].get(entity_type, 0)\n",
    "        })\n",
    "    return detailed_stats\n",
    "\n",
    "def main(input_folder, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "    splits = ['train', 'val', 'test']\n",
    "    all_stats = {}\n",
    "    summary_stats = []\n",
    "    detailed_stats = {split: [] for split in splits}\n",
    "    \n",
    "    for split in splits:\n",
    "        file_path = os.path.join(input_folder, f\"{split}.json\")\n",
    "        if os.path.exists(file_path):\n",
    "            data = load_data(file_path)\n",
    "            stats = analyze_data(data, tokenizer)\n",
    "            all_stats[split] = stats\n",
    "            summary_stats.append(collect_summary_statistics(stats, split))\n",
    "            detailed_stats[split].extend(collect_detailed_statistics(stats, split))\n",
    "        else:\n",
    "            print(f\"File {file_path} does not exist.\")\n",
    "    \n",
    "    # Create and save the summary table\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    summary_output_path = os.path.join(output_folder, f\"{os.path.basename(input_folder)}_summary.csv\")\n",
    "    summary_df.to_csv(summary_output_path, index=False)\n",
    "    \n",
    "    # Create and save the detailed tables for each split\n",
    "    detailed_dfs = {split: pd.DataFrame(detailed_stats[split]) for split in splits if detailed_stats[split]}\n",
    "    for split, df in detailed_dfs.items():\n",
    "        detailed_output_path = os.path.join(output_folder, f\"{os.path.basename(input_folder)}_{split}_detailed.csv\")\n",
    "        df.to_csv(detailed_output_path, index=False)\n",
    "    \n",
    "    # Create and save the panel as a single image\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    for i, split in enumerate(splits):\n",
    "        if split in all_stats:\n",
    "            stats = all_stats[split]\n",
    "            # Token Length Histogram\n",
    "            axes[0, i].hist(stats['token_lengths'], bins=20, edgecolor='black')\n",
    "            axes[0, i].set_title(f'Token Lengths in {split} Split')\n",
    "            axes[0, i].set_xlabel('Token Length')\n",
    "            axes[0, i].set_ylabel('Frequency')\n",
    "            \n",
    "            # Number of Entities Histogram\n",
    "            axes[1, i].hist(stats['num_entities'], bins=20, edgecolor='black')\n",
    "            axes[1, i].set_title(f'Number of Entities in {split} Split')\n",
    "            axes[1, i].set_xlabel('Number of Entities')\n",
    "            axes[1, i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Save the entire figure as one image\n",
    "    panel_output_path = os.path.join(output_folder, f\"{os.path.basename(input_folder)}_panel_histograms.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(panel_output_path, format='png')\n",
    "    plt.close()\n",
    "    \n",
    "    return summary_df, detailed_dfs\n",
    "\n",
    "# Define the input and output folders\n",
    "input_folder = \"./data/tac_chunked\"\n",
    "output_folder = \"./eda_output/tac_eda\"\n",
    "summary_df, detailed_dfs = main(input_folder, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
